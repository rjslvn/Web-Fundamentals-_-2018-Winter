{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPVABhXJ3gkBkmWOPc+G5nU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rjslvn/personal/blob/master/QnA_Bot_with_Embeddings_and_Scraping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# This code is designed to scrape a website, process the text, and then use OpenAI's API to answer questions based on the scraped text."
      ],
      "metadata": {
        "id": "WyOixNlfgRbv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Click the little Play button on the top left of each codeblock to get started (make sure to do it for all 13 steps)\n",
        "\n",
        "you may need to run these commands\n",
        "\n",
        "\n",
        "```\n",
        "!pip install bs4 tiktoken openai numpy pandas os pypdf2 requests tqdm \n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "_MMMUnM-iJXa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1: Import Necessary Libraries**\n",
        "\n",
        "This step imports all the necessary Python libraries that will be used in the script. These include libraries for handling PDF files, making HTTP requests, parsing HTML, manipulating data, and interacting with OpenAI's API."
      ],
      "metadata": {
        "id": "arAJzdKOiLoh"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vQ9hcHM4iIGF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mb8eETYNf1Iz"
      },
      "outputs": [],
      "source": [
        "################################################################################\n",
        "### Step 1\n",
        "################################################################################\n",
        "import PyPDF2\n",
        "from io import BytesIO\n",
        "import requests\n",
        "import re\n",
        "import urllib.request\n",
        "from bs4 import BeautifulSoup\n",
        "from collections import deque\n",
        "from html.parser import HTMLParser\n",
        "from urllib.parse import urlparse\n",
        "import os\n",
        "import pandas as pd\n",
        "import tiktoken\n",
        "import openai\n",
        "import numpy as np\n",
        "from openai.embeddings_utils import distances_from_embeddings, cosine_similarity\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "time.sleep(1)\n",
        "# Regex pattern to match a URL\n",
        "HTTP_URL_PATTERN = r'^https[*]://.+'\n",
        "openai.api_key = \"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\"\n",
        "# Define root domain to crawl\n",
        "# Prompt the user for domain input# Prompt the user for a start URL input\n",
        "start_url = input(\"Enter the start URL (e.g. https://en.wikipedia.org/wiki/Autoregressive_integrated_moving_average): \")\n",
        "url_obj = urlparse(start_url)\n",
        "domain = url_obj.netloc\n",
        "local_domain = domain\n",
        "\n",
        "# Create a class to parse the HTML and get the hyperlinks\n",
        "class HyperlinkParser(HTMLParser):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.hyperlinks = []\n",
        "\n",
        "    def handle_starttag(self, tag, attrs):\n",
        "        attrs = dict(attrs)\n",
        "        if tag == \"a\" and \"href\" in attrs:\n",
        "            self.hyperlinks.append(attrs[\"href\"])\n",
        "\n",
        "full_url = start_url\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2: Define Variables and Functions**\n",
        "\n",
        "Here, several variables and functions are defined. These include a regular expression pattern for matching URLs, an OpenAI API key, and a function for parsing hyperlinks in HTML."
      ],
      "metadata": {
        "id": "fkW2aUuYg-z5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "################################################################################\n",
        "### Step 2\n",
        "################################################################################\n",
        "\n",
        "# Function to get the hyperlinks from a URL\n",
        "def get_hyperlinks(url):\n",
        "    \n",
        "    # Try to open the URL and read the HTML\n",
        "    try:\n",
        "        # Open the URL and read the HTML\n",
        "        with urllib.request.urlopen(url) as response:\n",
        "\n",
        "            # If the response is not HTML, return an empty list\n",
        "            if not response.info().get('Content-Type').startswith(\"text/html\"):\n",
        "                return []\n",
        "            \n",
        "            # Decode the HTML\n",
        "            html = response.read().decode('utf-8')\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        return []\n",
        "\n",
        "    # Create the HTML Parser and then Parse the HTML to get hyperlinks\n",
        "    parser = HyperlinkParser()\n",
        "    parser.feed(html)\n",
        "\n",
        "    return parser.hyperlinks\n",
        "\n"
      ],
      "metadata": {
        "id": "RHbXSrzBhkaW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "PO7xObcbjyQ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3: Get Hyperlinks**\n",
        "\n",
        "This step defines a function to extract all hyperlinks from a given URL. It opens the URL, reads the HTML, and uses the HyperlinkParser class to extract all hyperlinks."
      ],
      "metadata": {
        "id": "nLp4E93DiNcR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "### Step 3\n",
        "################################################################################\n",
        "\n",
        "# Function to get the hyperlinks from a URL that are within the same domain\n",
        "def get_domain_hyperlinks(local_domain, url):\n",
        "    clean_links = []\n",
        "    for link in set(get_hyperlinks(url)):\n",
        "        clean_link = None\n",
        "\n",
        "        # If the link is a URL, check if it is within the same domain or subdomain\n",
        "        if re.search(HTTP_URL_PATTERN, link):\n",
        "            # Parse the URL and check if the domain or subdomain is the same\n",
        "            url_obj = urlparse(link)\n",
        "            if local_domain in url_obj.netloc:\n",
        "                clean_link = link\n",
        "\n",
        "        # If the link is not a URL, check if it is a relative link\n",
        "        else:\n",
        "            if link.startswith(\"/\"):\n",
        "                link = link[1:] #CHANGE THIS FOR DEPTH DEPTH DEPTH DEPTH DEPTH\n",
        "            elif link.startswith(\"#\") or link.startswith(\"mailto:\"):\n",
        "                continue\n",
        "            clean_link = \"https://\" + local_domain + \"/\" + link\n",
        "\n",
        "        if clean_link is not None:\n",
        "            if clean_link.endswith(\"/\"):\n",
        "                clean_link = clean_link[:-1]\n",
        "            clean_links.append(clean_link)\n",
        "\n",
        "    # Return the list of hyperlinks that are within the same domain or subdomain\n",
        "    return list(set(clean_links))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "IkXl_oMzhkh4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 4: Get Domain Hyperlinks**\n",
        "\n",
        "This function retrieves all hyperlinks from a given URL that are within the same domain. It cleans up the links and ensures they belong to the same domain or subdomain."
      ],
      "metadata": {
        "id": "0nU0cGdJjz10"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "################################################################################\n",
        "### Step 4\n",
        "################################################################################\n",
        "\n",
        "def sanitize_filename(filename):\n",
        "    invalid_chars = '\\\\/:*?\"<>|'\n",
        "    for char in invalid_chars:\n",
        "        filename = filename.replace(char, '_')\n",
        "    return filename\n",
        "def crawl(url):\n",
        "    # Parse the URL and get the domain\n",
        "    local_domain = urlparse(url).netloc\n",
        "\n",
        "    # Create a directory to store the text files\n",
        "    if not os.path.exists(\"text/\"):\n",
        "        os.mkdir(\"text/\")\n",
        "\n",
        "    if not os.path.exists(\"text/\" + local_domain + \"/\"):\n",
        "        os.mkdir(\"text/\" + local_domain + \"/\")\n",
        "\n",
        "    # Create a directory to store the csv files\n",
        "    if not os.path.exists(\"processed\"):\n",
        "        os.mkdir(\"processed\")\n",
        "\n",
        "    # Get the hyperlinks from the URL (one level deep)\n",
        "    one_level_links = get_domain_hyperlinks(local_domain, url)\n",
        "\n",
        "    # Process the initial URL\n",
        "    process_url(url, local_domain)\n",
        "\n",
        "    # Process each link one level deep\n",
        "    for link in one_level_links:\n",
        "        process_url(link, local_domain)\n",
        "\n",
        "def process_url(url, local_domain):\n",
        "    print(url)  # for debugging and to see the progress\n",
        "\n",
        "    if url.lower().endswith(\".pdf\"):\n",
        "        response = requests.get(url)\n",
        "        pdf_content = BytesIO(response.content)\n",
        "        try:\n",
        "            pdf_reader = PyPDF2.PdfReader(pdf_content)\n",
        "            pdf_text = \"\"\n",
        "            for page_num in range(len(pdf_reader.pages)):\n",
        "                pdf_text += pdf_reader.pages[page_num].extract_text()\n",
        "            save_text_to_file(url, local_domain, pdf_text)\n",
        "        except PyPDF2.errors.PdfReadError:\n",
        "            print(f\"Unable to parse PDF at {url}\")\n",
        "    else:\n",
        "        # Get the text from the URL using BeautifulSoup\n",
        "        soup = BeautifulSoup(requests.get(url).text, \"html.parser\")\n",
        "        text = soup.get_text()\n",
        "        save_text_to_file(url, local_domain, text)\n",
        "\n",
        "def save_text_to_file(url, local_domain, text):\n",
        "    # Save text from the url to a <url>.txt file\n",
        "    with open(\"text/\" + local_domain + \"/\" + sanitize_filename(url[8:]) + \".txt\", \"w\", encoding=\"UTF-8\") as f:\n",
        "        # If the crawler gets to a page that requires JavaScript, it will stop the crawl\n",
        "        if \"You need to enable JavaScript to run this app.\" in text:\n",
        "            print(\"Unable to parse page \" + url + \" due to JavaScript being required\")\n",
        "\n",
        "        # Otherwise, write the text to the file in the text directory\n",
        "        f.write(text)\n",
        "\n",
        "crawl(full_url)\n",
        "\n"
      ],
      "metadata": {
        "id": "XrDoiNR3hkop"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 5: Crawl the Website**\n",
        "\n",
        "This step involves crawling the website and processing each URL. It retrieves all the hyperlinks from the start URL and processes each one. The processing involves extracting text from the URL and saving it to a file."
      ],
      "metadata": {
        "id": "_zYISDKHiMwv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "################################################################################\n",
        "### Step 5\n",
        "################################################################################\n",
        "\n",
        "def remove_newlines(serie):\n",
        "    serie = serie.str.replace('\\n', ' ')\n",
        "    serie = serie.str.replace('\\\\n', ' ')\n",
        "    serie = serie.str.replace('  ', ' ')\n",
        "    serie = serie.str.replace('  ', ' ')\n",
        "    return serie\n",
        "\n",
        "\n",
        "################################################################################\n",
        "### Step 6\n",
        "################################################################################\n",
        "\n",
        "# Create a list to store the text files\n",
        "texts=[]\n",
        "\n",
        "# Get all the text files in the text directory\n",
        "for file in os.listdir(\"text/\" + domain + \"/\"):\n",
        "\n",
        "    # Open the file and read the text\n",
        "    with open(\"text/\" + local_domain + \"/\" + file, \"r\", encoding=\"UTF-8\") as f:\n",
        "        text = f.read()\n",
        "\n",
        "        # Omit the first 11 lines and the last 4 lines, then replace -, _, and #update with spaces.\n",
        "        texts.append((file[11:-4].replace('-',' ').replace('_', ' ').replace('#update',''), text))\n",
        "\n",
        "# Create a dataframe from the list of texts\n",
        "df = pd.DataFrame(texts, columns = ['fname', 'text'])\n",
        "\n",
        "#Here's the continuation of the corrected code:\n",
        "\n",
        "# Set the text column to be the raw text with the newlines removed\n",
        "df['text'] = df.fname + \". \" + remove_newlines(df.text)\n",
        "df.to_csv('processed/scraped.csv')\n",
        "df.head()\n",
        "\n"
      ],
      "metadata": {
        "id": "rCJCE12yhkvW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 6: Process the Text**\n",
        "\n",
        "This step processes the text that was retrieved from the website. It involves removing newline characters and splitting the text into chunks of a maximum number of tokens.\n",
        "\n",
        "stion, and then uses the previous steps to generate and print an answer."
      ],
      "metadata": {
        "id": "EGaaHirdkCN8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "### Step 7\n",
        "################################################################################\n",
        "\n",
        "# Load the cl100k_base tokenizer which is designed to work with the ada-002 model\n",
        "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
        "\n",
        "df = pd.read_csv('processed/scraped.csv', index_col=0)\n",
        "df.columns = ['title', 'text']\n",
        "\n",
        "# Tokenize the text and save the number of tokens to a new column\n",
        "df['n_tokens'] = df.text.apply(lambda x: len(tokenizer.encode(x)))\n",
        "\n",
        "# Visualize the distribution of the number of tokens per row using a histogram\n",
        "df.n_tokens.hist()\n",
        "\n"
      ],
      "metadata": {
        "id": "dRV8Gqi3hk09"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 7: Tokenize the Text**\n",
        "\n",
        "The text is tokenized (broken down into individual words or terms) and the number of tokens is saved to a new column in the dataframe.\n",
        "\n"
      ],
      "metadata": {
        "id": "iQo5dbtgkPb2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "### Step 8\n",
        "################################################################################\n",
        "\n",
        "max_tokens = 500\n",
        "\n",
        "# Function to split the text into chunks of a maximum number of tokens\n",
        "def split_into_many(text, max_tokens = max_tokens):\n",
        "\n",
        "    # Split the text into sentences\n",
        "    sentences = text.split('. ')\n",
        "\n",
        "    # Get the number of tokens for each sentence\n",
        "    n_tokens = [len(tokenizer.encode(\" \" + sentence)) for sentence in sentences]\n",
        "    \n",
        "    chunks = []\n",
        "    tokens_so_far = 0\n",
        "    chunk = []\n",
        "\n",
        "    # Loop through the sentences and tokens joined together in a tuple\n",
        "    for sentence, token in zip(sentences, n_tokens):\n",
        "\n",
        "        # If the number of tokens so far plus the number of tokens in the current sentence is greater \n",
        "        # than the max number of tokens, then add the chunk to the list of chunks and reset\n",
        "        # the chunk and tokens so far\n",
        "        if tokens_so_far + token > max_tokens:\n",
        "            chunks.append(\". \".join(chunk) + \".\")\n",
        "            chunk = []\n",
        "            tokens_so_far = 0\n",
        "\n",
        "        # If the number of tokens in the current sentence is greater than the max number of \n",
        "        # tokens, go to the next sentence\n",
        "        if token > max_tokens:\n",
        "            continue\n",
        "\n",
        "        # Otherwise, add the sentence to the chunk and add the number of tokens to the total\n",
        "        chunk.append(sentence)\n",
        "        tokens_so_far += token + 1\n",
        "        \n",
        "    # Add the last chunk to the list of chunks\n",
        "    if chunk:\n",
        "        chunks.append(\". \".join(chunk) + \".\")\n",
        "\n",
        "    return chunks\n",
        "    \n",
        "\n",
        "shortened = []\n",
        "\n",
        "# Loop through the dataframe\n",
        "for row in df.iterrows():\n",
        "\n",
        "    # If the text is None, go to the next row\n",
        "    if row[1]['text'] is None:\n",
        "        continue\n",
        "\n",
        "    # If the number of tokens is greater than the max number of tokens, split the text into chunks\n",
        "    if row[1]['n_tokens'] > max_tokens:\n",
        "        shortened += split_into_many(row[1]['text'])\n",
        "    \n",
        "    # Otherwise, add the text to the list of shortened texts\n",
        "    else:\n",
        "        shortened.append( row[1]['text'] )\n",
        "\n"
      ],
      "metadata": {
        "id": "r1p7xGYWhk5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 8: Split Text into Chunks**\n",
        "\n",
        "If the number of tokens in a text is greater than the maximum number of tokens, the text is split into chunks. Each chunk has a maximum number of tokens.\n"
      ],
      "metadata": {
        "id": "vxiIEjrjkRQo"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eZDOUOlYkRA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Step 9: Create a DataFrame**\n",
        "\n",
        "A DataFrame is created from the shortened texts. The number of tokens for each text is calculated and added to the DataFrame.\n"
      ],
      "metadata": {
        "id": "XYendPLskTMV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "### Step 9\n",
        "################################################################################\n",
        "\n",
        "df = pd.DataFrame(shortened, columns = ['text'])\n",
        "df['n_tokens'] = df.text.apply(lambda x: len(tokenizer.encode(x)))\n",
        "df.n_tokens.hist()\n",
        "\n"
      ],
      "metadata": {
        "id": "oWu0X2Ochk9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Step 10: Get Embeddings**\n",
        "\n",
        "This step involves getting embeddings for the text. Embeddings are a way of representing text in a numerical format that can be understood by machine learning models.\n"
      ],
      "metadata": {
        "id": "FpdV0jbZiPom"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "### Step 10\n",
        "################################################################################\n",
        "\n",
        "# Note that you may run into rate limit issues depending on how many files you try to embed\n",
        "# Please check out our rate limit guide to learn more on how to handle this: https://platform.openai.com/docs/guides/rate-limits\n",
        "\n",
        "df['embeddings'] = df.text.apply(lambda x: openai.Embedding.create(input=x, engine='text-embedding-ada-002')['data'][0]['embedding'])\n",
        "df.to_csv('processed/embeddings.csv')\n",
        "# Load your embeddings DataFrame\n",
        "df = pd.read_csv('processed/embeddings.csv')\n",
        "\n"
      ],
      "metadata": {
        "id": "S1G3tfL2hlCe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Step 11: Load Embeddings**\n",
        "\n",
        "The embeddings are loaded into the DataFrame.\n"
      ],
      "metadata": {
        "id": "WTTYEQkUiPcn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "### Step 11\n",
        "################################################################################\n",
        "\n",
        "# Load the embeddings from the csv file\n",
        "df = pd.read_csv('processed/embeddings.csv', index_col=0)\n",
        "df['embeddings'] = df.embeddings.apply(lambda x: np.array(eval(x)))\n",
        "# Convert the embeddings from strings to numpy arrays\n",
        "\n",
        "# Convert embeddings into a matrix for cosine similarity computation\n",
        "embeddings_matrix = np.vstack(df['embeddings'].values)\n",
        "\n",
        "print(embeddings_matrix.shape)\n",
        "\n",
        "# Calculate the cosine similarity between the embeddings\n",
        "def custom_cosine_similarity(a, b):\n",
        "    a = np.array(a)\n",
        "    b = np.array(b)\n",
        "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
        "\n",
        "df['similarity'] = df.embeddings.apply(lambda x: custom_cosine_similarity(df.embeddings[0], x))\n",
        "\n",
        "# Sort the dataframe by similarity\n",
        "df = df.sort_values('similarity', ascending=False)\n",
        "\n",
        "# Print the top 5 most similar texts\n",
        "print(df.head(5))\n",
        "\n"
      ],
      "metadata": {
        "id": "v1XkOv2AhlGX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Step 12: Answer Questions**\n",
        "\n",
        "This step involves answering questions based on the most similar context from the DataFrame texts. It uses OpenAI's API to generate the answers.\n",
        "\n"
      ],
      "metadata": {
        "id": "MPybH5JQiPjk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "### Step 12\n",
        "################################################################################\n",
        "\n",
        "# Save the dataframe to a csv file\n",
        "df.to_csv('processed/similarities.csv')\n",
        "\n",
        "# Print a message to indicate that the process is complete\n",
        "print(\"Process complete. The similarities have been saved to 'processed/similarities.csv'.\")\n",
        "\n",
        "# Load the embeddings from the CSV file\n",
        "df = pd.read_csv('processed/embeddings.csv', index_col=0)\n",
        "\n",
        "\n",
        "# Convert the embeddings from strings to numpy arrays\n",
        "df['embeddings'] = df.embeddings.apply(lambda x: np.array(eval(x)))\n",
        "\n",
        "# Convert embeddings into a matrix for cosine similarity computation\n",
        "embeddings_matrix = np.vstack(df['embeddings'].values)\n",
        "\n",
        "# Function to get most similar documents to a query\n",
        "def get_similar_documents(query_embedding, embeddings_matrix, top_n=5):\n",
        "    query_embedding = np.array(query_embedding)\n",
        "    similarity_scores = cosine_similarity(query_embedding.reshape(1, -1), embeddings_matrix.T) # This line has been corrected\n",
        "    sorted_indices = np.argsort(similarity_scores[0])[::-1]\n",
        "    return sorted_indices[:top_n]\n",
        "\n",
        "# Function to get the query embedding\n",
        "def get_query_embedding(query):\n",
        "    return openai.Embedding.create(input=query, engine='text-embedding-ada-002')['data'][0]['embedding']\n",
        "\n",
        "def answer_question(df, question, model=\"text-davinci-003\", max_len=1800, max_tokens=150, stop_sequence=None, debug=False):\n",
        "    \"\"\"\n",
        "    Answer a question based on the most similar context from the dataframe texts or provide an open-ended response\n",
        "    \"\"\"\n",
        "    context = create_context(\n",
        "        question,\n",
        "        df,\n",
        "        max_len=max_len,\n",
        "    )\n",
        "    # If debug, print the raw model response\n",
        "    if debug:\n",
        "        print(\"Context:\\n\" + context)\n",
        "        print(\"\\n\\n\")\n",
        "\n",
        "    try:\n",
        "        # Create a completions using the question and context\n",
        "        response = openai.Completion.create(\n",
        "            engine=model,\n",
        "            prompt=f\"Answer the question based on the context below, and if the question can't be answered based on the context, provide an open-ended response.\\n\\nContext: {context}\\n\\n---\\n\\nQuestion: {question}\\nAnswer:\",\n",
        "            temperature=0.5,\n",
        "            max_tokens=max_tokens,\n",
        "            top_p=1,\n",
        "            frequency_penalty=0,\n",
        "            presence_penalty=0,\n",
        "            stop=stop_sequence,\n",
        "        )\n",
        "        return response.choices[0].text.strip()\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        return \"\"\n",
        "\n",
        "# Replace \"your_openai_api_key\" with your actual OpenAI API key\n",
        "\n",
        "def create_context(\n",
        "    question, df, max_len=1800, size=\"ada\"\n",
        "):\n",
        "    \"\"\"\n",
        "    Create a context for a question by finding the most similar context from the dataframe\n",
        "    \"\"\"\n",
        "\n",
        "    # Get the embeddings for the question\n",
        "    q_embeddings = openai.Embedding.create(input=question, engine='text-embedding-ada-002')['data'][0]['embedding']\n",
        "\n",
        "    # Get the distances from the embeddings\n",
        "    df['distances'] = distances_from_embeddings(q_embeddings, df['embeddings'].values, distance_metric='cosine')\n",
        "\n",
        "\n",
        "    returns = []\n",
        "    cur_len = 0\n",
        "\n",
        "    # Sort by distance and add the text to the context until the context is too long\n",
        "    for i, row in df.sort_values('distances', ascending=True).iterrows():\n",
        "        \n",
        "        # Add the length of the text to the current length\n",
        "        cur_len += row['n_tokens'] + 4\n",
        "        \n",
        "        # If the context is too long, break\n",
        "        if cur_len > max_len:\n",
        "            break\n",
        "        \n",
        "        # Else add it to the text that is being returned\n",
        "        returns.append(row[\"text\"])\n",
        "\n",
        "    # Return the context\n",
        "    return \"\\n\\n###\\n\\n\".join(returns)\n",
        "\n"
      ],
      "metadata": {
        "id": "8khdFwk1hlOn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 13: Interactive Question-Answering**\n",
        "\n",
        "This final step allows for interactive question-answering. It prompts the user to enter a que"
      ],
      "metadata": {
        "id": "vG4J3cIriPmw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "################################################################################\n",
        "### Step 13\n",
        "################################################################################\n",
        "conversation_history = \"\"\n",
        "\n",
        "while True:\n",
        "    user_question = input(\"Enter your question (type 'exit' to quit): \")\n",
        "    if user_question.lower() == 'exit':\n",
        "        break\n",
        "    \n",
        "    conversation_history += f\"\\n\\nQuestion: {user_question}\"\n",
        "    answer = answer_question(df, question=user_question, debug=False)\n",
        "    conversation_history += f\"\\nAnswer: {answer}\"\n",
        "    print(answer)\n"
      ],
      "metadata": {
        "id": "4S5BGahfhlUQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}